{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa3acf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, AutoModel, BertForSequenceClassification\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from transformers import TextClassificationPipeline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1562d187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af8f8a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>is_obligation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14712</th>\n",
       "      <td>In either event, Tenant shall also pay any oth...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8318</th>\n",
       "      <td>The Company will pay any personal tax liabilit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9773</th>\n",
       "      <td>Not less than thirty (30) days prior to the ex...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4596</th>\n",
       "      <td>Unless otherwise agreed by the Administrative ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9426</th>\n",
       "      <td>Landlord also acknowledges and agrees that Ten...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>FORCE MAJEURE: Neither Lindows.com nor License...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10697</th>\n",
       "      <td>EMPLOYEE BENEFITS: During the Term of Employme...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7693</th>\n",
       "      <td>For each fiscal year of the Company during the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1365</th>\n",
       "      <td>requirement of any government agency, or any o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12129</th>\n",
       "      <td>If any portion of the Work Product is not rule...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  is_obligation\n",
       "14712  In either event, Tenant shall also pay any oth...              1\n",
       "8318   The Company will pay any personal tax liabilit...              1\n",
       "9773   Not less than thirty (30) days prior to the ex...              1\n",
       "4596   Unless otherwise agreed by the Administrative ...              0\n",
       "9426   Landlord also acknowledges and agrees that Ten...              0\n",
       "348    FORCE MAJEURE: Neither Lindows.com nor License...              0\n",
       "10697  EMPLOYEE BENEFITS: During the Term of Employme...              0\n",
       "7693   For each fiscal year of the Company during the...              1\n",
       "1365   requirement of any government agency, or any o...              0\n",
       "12129  If any portion of the Work Product is not rule...              0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/obligation_extraction_df.csv')\n",
    "# df['Clause Text'] = df['Clause Text'].apply(lambda x: process_text(x))\n",
    "df['is_obligation'] = df['is_obligation'].apply(lambda x: int(x))\n",
    "\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8a78a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint-1000  checkpoint-4000  checkpoint-6500  pytorch_model.bin\r\n",
      "checkpoint-1500  checkpoint-4500  checkpoint-7000  special_tokens_map.json\r\n",
      "checkpoint-2000  checkpoint-500   checkpoint-7500  tokenizer.json\r\n",
      "checkpoint-2500  checkpoint-5000  checkpoint-8000  tokenizer_config.json\r\n",
      "checkpoint-3000  checkpoint-5500  checkpoint-8500  training_args.bin\r\n",
      "checkpoint-3500  checkpoint-6000  config.json\t   vocab.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../models/legal_bert_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8830ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 512, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 512)\n",
       "      (token_type_embeddings): Embedding(2, 512)\n",
       "      (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=512, out_features=27, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('distilroberta-base', model_max_length=512)\n",
    "model = BertForSequenceClassification.from_pretrained('distilroberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f26781",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "271b67d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df['Clause Text'].iloc[100]\n",
    "text = process_text(text)\n",
    "\n",
    "tokens = tokenizer.encode_plus(\n",
    "     text,\n",
    "     add_special_tokens=True,\n",
    "     truncation = True,\n",
    "     padding=\"max_length\",\n",
    "     return_attention_mask=True,\n",
    "     return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b299dacf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 1940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11/1280511914.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  softmax_res = softmax(logits)  # Needed for confidence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 of 1940\n",
      "2 of 1940\n",
      "3 of 1940\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11/1280511914.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtext_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_chunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1520\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1522\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1523\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    989\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m         )\n\u001b[0;32m--> 991\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    992\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    580\u001b[0m                 )\n\u001b[1;32m    581\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    583\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    508\u001b[0m             \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attn_present_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[1;32m    511\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m   2184\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2186\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from math import ceil\n",
    "\n",
    "CHUNK_SIZE = 8  # TODO what is the optimal value\n",
    "num_examples = len(df['Clause Text'].values)\n",
    "num_chunks = ceil(num_examples / CHUNK_SIZE)\n",
    "\n",
    "text_chunks = np.array_split(df['Clause Text'].values, num_chunks)\n",
    "predictions = []\n",
    "del df\n",
    "\n",
    "for idx, text_chunk in enumerate(text_chunks):\n",
    "    print(f'{idx} of {num_chunks}')    \n",
    "    \n",
    "    text_chunk = list(text_chunk)\n",
    "    tokens = tokenizer(text_chunk, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    res = model(**tokens)\n",
    "\n",
    "    for logits in res[0]:\n",
    "        softmax_res = softmax(logits)  # Needed for confidence\n",
    "        index = np.argsort(softmax_res.detach().numpy())[-1]\n",
    "\n",
    "        predictions.append(labels[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ac72d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "65099236",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ = [p if p != 'payment' else 'payment_terms' for p in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9c7e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "acc54958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               precision    recall  f1-score   support\n",
      "\n",
      "                          UNK       0.00      0.00      0.00         0\n",
      "                   assignment       0.85      0.78      0.81       212\n",
      "              confidentiality       0.79      0.90      0.84       260\n",
      "    data_security_and_privacy       0.51      0.91      0.66        43\n",
      "                  definitions       0.99      0.76      0.86      3957\n",
      "           dispute_resolution       0.93      0.86      0.90       175\n",
      "             entire_agreement       0.91      0.95      0.93       136\n",
      "          export_control_laws       0.57      0.84      0.68        25\n",
      "                force_majeure       0.77      0.98      0.86        54\n",
      "                governing_law       0.96      0.96      0.96       140\n",
      "              indemnification       0.83      0.59      0.69       418\n",
      "                    insurance       0.00      0.00      0.00         0\n",
      "        intellectual_property       0.29      0.67      0.40       116\n",
      "                 introduction       0.00      0.00      0.00         0\n",
      "           limit_of_liability       0.78      0.51      0.62       301\n",
      "                      notices       0.95      0.46      0.61      1180\n",
      "                payment_terms       0.86      0.56      0.68       924\n",
      "                     preamble       0.99      0.77      0.86       661\n",
      "  relationship_of_the_parties       0.70      0.84      0.76        62\n",
      "representation_and_warranties       0.97      0.68      0.80      3582\n",
      "                 severability       0.98      0.98      0.98       118\n",
      "                    signature       0.94      0.66      0.77      1998\n",
      "                     survival       0.68      0.95      0.79       121\n",
      "                        taxes       0.50      0.90      0.65       193\n",
      "                         term       0.81      0.67      0.73       243\n",
      "                  termination       0.90      0.68      0.78       560\n",
      "    third_party_beneficiaries       0.52      0.91      0.66        34\n",
      "\n",
      "                     accuracy                           0.69     15513\n",
      "                    macro avg       0.70      0.70      0.68     15513\n",
      "                 weighted avg       0.93      0.69      0.78     15513\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df.Label.values, predictions_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574f52ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# worse - notices, entire agreement, survival, force_majeure\n",
    "\n",
    "# drastically better - termination, term, payment_terms\n",
    "# drastically worse - signature, data_security_and_privacy, third_party_beneficiaries, export_control_laws(!), relationship_of_the_parties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df476475",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_ALTERNATIVE_PREDICTIONS = 3\n",
    "ALTERNATIVE_PREDICTION_PROBABILITY_THRESHOLD = 0.0001\n",
    "LABEL_DECODER = label_decoder\n",
    "\n",
    "HIGH_CONFIDENCE_THRESHOLD = 0.9\n",
    "MEDIUM_CONFIDENCE_THRESHOLD = 0.8\n",
    "LOW_CONFIDENCE_THRESHOLD = 0.7\n",
    "\n",
    "LOW_CONFIDENCE = 'low confidence'\n",
    "MEDIUM_CONFIDENCE = 'medium confidence'\n",
    "HIGH_CONFIDENCE = 'high confidence'\n",
    "\n",
    "class ClauseIdService:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def calculate_confidence_level(self, confidence):\n",
    "        if confidence >= HIGH_CONFIDENCE_THRESHOLD:\n",
    "            return HIGH_CONFIDENCE\n",
    "        elif confidence >= MEDIUM_CONFIDENCE_THRESHOLD:\n",
    "            return MEDIUM_CONFIDENCE\n",
    "\n",
    "        return LOW_CONFIDENCE\n",
    "    \n",
    "    def _transform_pred(self, prediction):\n",
    "        category = LABEL_DECODER[prediction['label']]  # should this be passed through init?\n",
    "        confidence = prediction['score']\n",
    "\n",
    "        return category, confidence\n",
    "    \n",
    "    def _get_indexes(self, prediction):\n",
    "        indexes = np.argsort([pair['score'] for pair in prediction])\n",
    "        # sort indexes in descending order\n",
    "        indexes = indexes[::-1] \n",
    "        indexes = indexes[:NUMBER_OF_ALTERNATIVE_PREDICTIONS+1]\n",
    "        \n",
    "        return indexes\n",
    "            \n",
    "    def _get_result(self, predictions, paragraphs):\n",
    "        result = dict()\n",
    "        \n",
    "        for position, prediction in enumerate(predictions):\n",
    "            indexes = self._get_indexes(prediction)\n",
    "            \n",
    "            transformed_predictions = [self._transform_pred(prediction[idx]) for idx in indexes]\n",
    "            clause_category, confidence = transformed_predictions[0]\n",
    "\n",
    "            alternate_predictions = [\n",
    "                {\"clause category\": category, \"confidence\": confidence} \n",
    "                for category, confidence in transformed_predictions[1:]\n",
    "                if confidence > ALTERNATIVE_PREDICTION_PROBABILITY_THRESHOLD\n",
    "            ]\n",
    "\n",
    "            result[str(position)] = {\n",
    "                'clause': paragraphs[position],\n",
    "                'clause category': clause_category,\n",
    "                'confidence': confidence,\n",
    "                'confidence_level': self.calculate_confidence_level(confidence),\n",
    "                'alternate': alternate_predictions\n",
    "            }\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    def annotate(self, paragraphs):\n",
    "        predictions = self.model(paragraphs)\n",
    "        \n",
    "        return self._get_result(predictions, paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0eb0f0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = TextClassificationPipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_all_scores=True,\n",
    "    function_to_apply='softmax',\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1418b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': {'clause': ' This Fourth Amendment (the \"Fourth Amendment\") to that certain AFFILIATION AGREEMENT FOR DBS SATELLITE EXHIBITION OF CABLE PROGRAMMING dated as of November 15, 1993 by and between Playboy Entertainment Group, Inc. (\"Programmer\") and DirecTV, Inc. (\"Affiliate\"), as amended and supplemented, including by that certain First Amendment dated as of April 19, 1994, that certain Second Amendment dated July 26, 1995, and that certain Third Amendment dated August 26, 1997 (such Affiliation Agreement, as amended and supplemented, is referred to as the \"Agreement\"), is made and entered into as of March 15, 1999, with reference to the following facts (all defined terms used in this Fourth Amendment but not defined in this Fourth Amendment are defined in the Agreement):',\n",
       "  'clause category': 'introduction',\n",
       "  'confidence': 0.9997456669807434,\n",
       "  'confidence_level': 'high confidence',\n",
       "  'alternate': [{'clause category': 'UNK',\n",
       "    'confidence': 0.00017905904678627849}]},\n",
       " '1': {'clause': ' A. Affiliate currently exercises reasonable commercial efforts to maximize the number of Service Subscribers and Gross Receipts. Such efforts currently include exhibiting in excess of four hundred (400) thirty (30)-second cross-channel promotional advertising spots (the \"Cross-Channel Spots\") for each month of the Term, and in excess of Two Thousand Four Hundred (2,400) thirty (30) and/or sixty (60) second promotional advertising spots for each month of the Term on Affiliate\\'s pay-per-view preview channel (the \"Preview Channel Spots\"), all at no charge to Programmer. Affiliate and Programmer mutually determine in their reasonable business judgment the other Affiliate channels over which to exhibit the Cross-Channel Spots, based on whether such other channels have a likely target audience for the Service and are not channels where the Cross-Channel Spots might be objectionable to the DirecTV viewing audience. Such channel selection for the Cross-Channel Spots also is subject to third party programmers\\' limitations on adult-oriented promotional spots.',\n",
       "  'clause category': 'UNK',\n",
       "  'confidence': 0.9998871088027954,\n",
       "  'confidence_level': 'high confidence',\n",
       "  'alternate': []}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl = ClauseIdService(pipeline)\n",
    "\n",
    "cl.annotate(list(df['Clause Text'].values[0:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "8f3bc402",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pipeline(list(df['Clause Text'].values), return_all_scores=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "f46b3e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = [label_decoder[p['label']] for p in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "aff29bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               precision    recall  f1-score   support\n",
      "\n",
      "                          UNK       0.00      0.00      0.00         0\n",
      "                   assignment       0.85      0.81      0.83       212\n",
      "              confidentiality       0.75      0.87      0.81       260\n",
      "    data_security_and_privacy       0.43      0.88      0.58        43\n",
      "                  definitions       0.99      0.76      0.86      3957\n",
      "           dispute_resolution       0.90      0.86      0.88       175\n",
      "             entire_agreement       0.90      0.95      0.92       136\n",
      "          export_control_laws       0.53      0.84      0.65        25\n",
      "                force_majeure       0.71      0.98      0.82        54\n",
      "                governing_law       0.93      0.97      0.95       140\n",
      "              indemnification       0.82      0.58      0.68       418\n",
      "                    insurance       0.00      0.00      0.00         0\n",
      "        intellectual_property       0.29      0.71      0.41       116\n",
      "                 introduction       0.00      0.00      0.00         0\n",
      "           limit_of_liability       0.82      0.52      0.64       301\n",
      "                      notices       0.94      0.45      0.61      1180\n",
      "                payment_terms       0.86      0.56      0.68       924\n",
      "                     preamble       0.98      0.75      0.85       661\n",
      "  relationship_of_the_parties       0.61      0.82      0.70        62\n",
      "representation_and_warranties       0.96      0.66      0.79      3582\n",
      "                 severability       0.97      0.98      0.98       118\n",
      "                    signature       0.92      0.62      0.74      1998\n",
      "                     survival       0.70      0.93      0.80       121\n",
      "                        taxes       0.50      0.90      0.64       193\n",
      "                         term       0.78      0.71      0.75       243\n",
      "                  termination       0.85      0.73      0.79       560\n",
      "    third_party_beneficiaries       0.54      0.88      0.67        34\n",
      "\n",
      "                     accuracy                           0.69     15513\n",
      "                    macro avg       0.69      0.69      0.67     15513\n",
      "                 weighted avg       0.92      0.69      0.78     15513\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "predictions_ = [p if p != 'payment' else 'payment_terms' for p in pred_labels]\n",
    "\n",
    "print(classification_report(df.Label.values, predictions_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7421cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "                          UNK       0.00      0.00      0.00         0\n",
    "                   assignment       0.85      0.78      0.81       212\n",
    "              confidentiality       0.79      0.90      0.84       260\n",
    "    data_security_and_privacy       0.51      0.91      0.66        43\n",
    "                  definitions       0.99      0.76      0.86      3957\n",
    "           dispute_resolution       0.93      0.86      0.90       175\n",
    "             entire_agreement       0.91      0.95      0.93       136\n",
    "          export_control_laws       0.57      0.84      0.68        25\n",
    "                force_majeure       0.77      0.98      0.86        54\n",
    "                governing_law       0.96      0.96      0.96       140\n",
    "              indemnification       0.83      0.59      0.69       418\n",
    "                    insurance       0.00      0.00      0.00         0\n",
    "        intellectual_property       0.29      0.67      0.40       116\n",
    "                 introduction       0.00      0.00      0.00         0\n",
    "           limit_of_liability       0.78      0.51      0.62       301\n",
    "                      notices       0.95      0.46      0.61      1180\n",
    "                payment_terms       0.86      0.56      0.68       924\n",
    "                     preamble       0.99      0.77      0.86       661\n",
    "  relationship_of_the_parties       0.70      0.84      0.76        62\n",
    "representation_and_warranties       0.97      0.68      0.80      3582\n",
    "                 severability       0.98      0.98      0.98       118\n",
    "                    signature       0.94      0.66      0.77      1998\n",
    "                     survival       0.68      0.95      0.79       121\n",
    "                        taxes       0.50      0.90      0.65       193\n",
    "                         term       0.81      0.67      0.73       243\n",
    "                  termination       0.90      0.68      0.78       560\n",
    "    third_party_beneficiaries       0.52      0.91      0.66        34\n",
    "\n",
    "                     accuracy                           0.69     15513\n",
    "                    macro avg       0.70      0.70      0.68     15513\n",
    "                 weighted avg       0.93      0.69      0.78     15513'''\n",
    "\n",
    "'''                          \n",
    "                   assignment       0.85      0.81      0.83       212\n",
    "              confidentiality       0.75      0.87      0.81       260\n",
    "    data_security_and_privacy       0.43      0.88      0.58        43\n",
    "                  definitions       0.99      0.76      0.86      3957\n",
    "           dispute_resolution       0.90      0.86      0.88       175\n",
    "             entire_agreement       0.90      0.95      0.92       136\n",
    "          export_control_laws       0.53      0.84      0.65        25\n",
    "                force_majeure       0.71      0.98      0.82        54\n",
    "                governing_law       0.93      0.97      0.95       140\n",
    "              indemnification       0.82      0.58      0.68       418\n",
    "                    insurance       0.00      0.00      0.00         0\n",
    "        intellectual_property       0.29      0.71      0.41       116\n",
    "                 introduction       0.00      0.00      0.00         0\n",
    "           limit_of_liability       0.82      0.52      0.64       301\n",
    "                      notices       0.94      0.45      0.61      1180\n",
    "                payment_terms       0.86      0.56      0.68       924\n",
    "                     preamble       0.98      0.75      0.85       661\n",
    "  relationship_of_the_parties       0.61      0.82      0.70        62\n",
    "representation_and_warranties       0.96      0.66      0.79      3582\n",
    "                 severability       0.97      0.98      0.98       118\n",
    "                    signature       0.92      0.62      0.74      1998\n",
    "                     survival       0.70      0.93      0.80       121\n",
    "                        taxes       0.50      0.90      0.64       193\n",
    "                         term       0.78      0.71      0.75       243\n",
    "                  termination       0.85      0.73      0.79       560\n",
    "    third_party_beneficiaries       0.54      0.88      0.67        34\n",
    "\n",
    "                     accuracy                           0.69     15513\n",
    "                    macro avg       0.69      0.69      0.67     15513\n",
    "                 weighted avg       0.92      0.69      0.78     15513'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7a752a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import TextClassificationPipeline\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('../models/legal_bert_small', model_max_length=512)\n",
    "model = BertForSequenceClassification.from_pretrained('../models/legal_bert_small')\n",
    "model.eval()\n",
    "\n",
    "pipeline = TextClassificationPipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_all_scores=True,\n",
    "    function_to_apply='softmax',\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbeecc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f674d332",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_pickle('../data/clause_id/2019-11-19_clause_id_s1_s3_dataset.pkl')\n",
    "\n",
    "df2 = pd.read_pickle('../data/clause_id/all_clause_ids_data_frame.pkl')\n",
    "df2['Label'] = df2['clause_id'].apply(lambda x: LABEL_DICT[str(x)])\n",
    "\n",
    "df_train = pd.read_excel('../data/clause_id/clause_id_train.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea64714d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['Clause Text'].values, \n",
    "    df['label_id'].values, \n",
    "    test_size=.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e43586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca79f17b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c37f20e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
